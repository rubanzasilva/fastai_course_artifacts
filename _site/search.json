[
  {
    "objectID": "lesson_9.html#theory-part-1",
    "href": "lesson_9.html#theory-part-1",
    "title": "Stable Diffusion from the foundations",
    "section": "Theory — Part 1",
    "text": "Theory — Part 1\nBelow, I shall go through the theoretical concepts and show the inner workings of the Stable Diffusion algorithm. The work shown here is based on lesson 9 and this blog post by Rekil Prashanth, whose work really helped me understand the concepts I go through below of the practical deep learning for coders course.\nSo nothing here is really my invention, but rather a sort of summary to remind me of the concepts in a quick dive. So let’s start off.\n\nMagic function, f.\nSo we want to create a function that can be used to generate handwritten digits. Now how would we go about doing that?\nSo imagine that we have a magic function, f which, when passed in an image, returns the probability of that image being a handwritten digit. For example, if we pass in an image, X1, we get back a probability score of that being a handwritten digit. The images are created by drawing a number and putting some random noise on top of it.\nPassing in an image with more noise returns a lower probability. So now, how can we adjust our image such that we improve the probability that it is a handwritten digit?\n\n\n\nmagic function f\n\n\nWe need to find a way to adjust the image in that it improves the image, for example, in X3, where our number is a 1, we know that 1 does not have black pixels in the bottom right corner, so removing those should improve the probability that it is a handwritten digit.\nSo let’s run with that, for each image, we can adjust the pixels of the image such that the probability of it being a handwritten digit increases.\nIn the end, we have a way to find out in which direction and by which magnitude we can adjust our pixel values in order to improve the probability of an image being a handwritten digit. We want the gradient of the probability that X3 is a handwritten digit with respect to the pixels of x3 ( ∇p(X3)/∇X3 ). For every single pixel, the rate of change with respect to the pixel values can be expressed as ∂p(x3)/∂(x3) but when we do this for every pixel the ∂ becomes a nabla, ∇.\nAt the end of each step, we should have a less noisy image with an improved probability score, which we can then use as the input.\nWe can now compute the gradient with respect to every pixel and adjust every pixel by subtracting the gradient multiplied by a small learning rate, which we can call lr from the original pixel value.\n\nnew_pixel_value (original_pixel_value - (gradient * lr))\n\nThe learning rate helps control the size of the steps we take. As we shall see later on, denoising an image is an iterative process, and it does not provide good results if we try to do it in one shot.\nSo we now have a function that can turn any noisy input into a handwritten digit. We are currently passing in each pixel one at a time to calculate the derivative by finite differencing. We can do this at once using analytical derivatives by calling f.backward(), then X3.grad. This is a faster and more efficient way of getting the gradients.\n\nTraining a Unet\nWe now have a magic function, f which tells us which pixels to change and by how much to make any noisy input look more like a handwritten digit.\nSo now we need to create such a function. Before we do this, we need some training data. We can do this by getting images of handwritten digits and chucking some random noise on top of them like in the image below\n\n\n\nTraining Data\n\n\nRather than come up with a probability score for the amount of noise in an image, let’s say the amount of noise tells us how much an image is like a handwritten digit. Something with little amounts of noise looks more like a handwritten digit and vice versa.\nAs in the image above, we can see that a noisy image is just the handwritten digit plus the noise; for example, the number 7 is just the actual number 7 plus some noise.\nSo if we pass in a noisy image to our function, it should output the amount of noise; we then calculate the MSE between the predicted output and the original noise values, whose score we want to minimize.\nSubtracting the predicted noise from the original noise values leaves us with a less noisy image, which acts as the next input for our function, f. We shall call the above function a Unet."
  },
  {
    "objectID": "lesson_9.html#enter-the-variational-autoencoder-vae.",
    "href": "lesson_9.html#enter-the-variational-autoencoder-vae.",
    "title": "Stable Diffusion from the foundations",
    "section": "Enter the Variational AutoEncoder (VAE).",
    "text": "Enter the Variational AutoEncoder (VAE).\nNow, our current image above is 28 x 28, which results in 784 pixel values. Now most modern-day photos have dimensions in the thousands; for example, a 6 MB screenshot on my phone is 1125 x 2436, which results in 2,740,500 different pixel values, and this is just one image, yet we need a number of images to do proper training.\nTo help deal with this complexity, we can push our image from the pixel space into the latent space. This helps reduce the image size while still retaining most of the information from the image. This significantly improves the time for our Unet training, as it operates on image latents, which are significantly smaller.\nWe can do this with an encoder and a decoder, as we shall see below.\nIn our encoder, we take the images and reduce the number of features using a stride 2 convolution while increasing the number of channels. Finally, at the end, we add a block to squish down the number of channels. We can now do the inverse process using the decoder; we can calculate the MSE between the output and the input and try to minimize this.\nAt first, we get random outputs, but at the end of the training, we should get back something that looks like our input.\nThis gives us a model that can take an image and compress it into its latent representation; we can pass this latent representation to our Unet. Our Unet then outputs the prediction of noise given noisy image latents.\nThe VAE compression works because, as we see above, we have a way to get the image encoded and get back something similar when passed through the decoder. It works because images have a lot of details that are not necessarily needed.\nSo the VAE learns patterns in the data that make up an image, like edges, etc. Once it learns the essence/representation that preserves the structure and style well enough while ignoring what isn’t necessarily needed, it is able to learn to compress an image using only the relevant details, but enough so that it can easily map noisy images to images.\nNow that we have our noise predictions, we can subtract the noise from the noisy image latents, leaving us with less noisy latents and later denoised latents, which we can convert back into an approximation of the denoised image by passing them to our decoder.\nLet us illustrate this with the image from lesson 9 of the class that shows a 512 x 512 RGB image which has 786,432 (512 x 512 x 3) pixel values.\n\n\n\nenc_dec.png\n\n\nNow if you look above, you will see that we went from 512 x 512 pixel image to 64 x 64 image reprsentation which we can still use to train our Unet. This is compression by a factor of 8.\n\nEnter the Contrastive Language Image pre-training model (CLIP)\nSo if you notice when using Stable Diffusion models, many times you pass in an actual text prompt to sort of guide the model. So below, let’s see how we can get our model to take a text prompt as additional input.\nLet’s start with our example above. Say we want to get our image to generate a handwritten digit for the number 3. We would want to have a way to pass in our text alongside the noise as input to our Unet.\nThis can be done by passing in a one-hot encoding version of the number. With this extra information of what the original image was, the Unet should be able to do a better job of predicting the noise. We call this guidance.\nUnfortunately, one-hot encoding may not be a robust way of doing this; imagine if we had a text such as “A cute kitten.” To one-hot encode this, we would have to create a representation for every single word, which seems quite inefficient.\nThere is an alternative workaround: we can create a model that takes an image and a text description of the image and creates embeddings for the text that are close to the embeddings for its image, representing what the image looks like.\nWe start by downloading image and caption/tag pairs. We then pass the image to an image encoder and the text to a text encoder; both of them return embeddings that represent their input.\nI now calculate the similarity by taking the dot product of both embedding vectors; the higher the score, the more similar the embeddings, which is what we want.\nWe now put the images and text in a grid as shown below, where we can put the pair embeddings. We want only the diagonals for the matching pairs to have a high score, so we create a loss function that takes the matching pair embeddings and subtracts non-matching pair embeddings. Likewise, we then aim to maximize this, as we can see below.\n\n\n\nJeremy’s Original Example\n\n\nThis loss is called contrastive loss, and the pair of models that we use to calculate our embeddings are called CLIP (Contrastive Language-Image Pre-training)\nSo now if we pass in similar text captions such as “A cute cat,” “A cute kitten,” and “A furry kitten with a nice coating,” they should all give similar embeddings. We can now pass the text captions to our model, and it returns the embeddings, which are then passed in alongside the noisy latents to the Unet."
  },
  {
    "objectID": "lesson_9.html#time-steps",
    "href": "lesson_9.html#time-steps",
    "title": "Stable Diffusion from the foundations",
    "section": "Time steps",
    "text": "Time steps\nWe usually add varying amounts of noise to the the images, based of a noise schedule function. We can create something like this with an x -axis, t that ranges from 1 to 1000 and a y-axis that represents the amount of noise, something like what we have below\n\n\n\nNoise Scheduler Function.png\n\n\nSuch scheduler functions are usually decreasing, so what we do is pick a random number from our schedule function and use the noise value at that point for our selected image. This is usually referred to as the time step.\nPicking the value where t=0, meaning pure noise, results in a very bad prediction. This is because our Unet is trained to deal with images that have some bits of random noise, not total noise. This is because we never actually pass our model pure noise as input during training, so the model would not know how to deal with such a situation.\nThink of it like navigating in the dark; it would be impossible to know which direction to move in to reach a certain point, but if you have a bit of light in the direction, you can slowly get to the final destination.\nSo denoising an image turns out to be an iterative process, where you slowly move towards your goal by subtracting a little bit of noise at each point, ending up with a slightly less noisy image at every step until you reach something that looks like the image you are expecting. This is why we multiply our predicted gradients by the learning rate, which ensures that we take small steps in the right direction as opposed to trying to do the diffusion process in one step.\nIn every mini batch, you choose a random amount of noise or pick a point on the noise schedule and assign the y-axis value at that point as the noise. We can then use those varying amounts of noise to create the noisy images that we pass to our Unet to train it to predict the amount of noise. Adding the noise and subtracting the noise is done by the scheduler.\nAnother way of looking at this is we can actually just use the exact noise we added to the image; doing this would mean we wouldn’t have to pass in t. With this, the diffusers stop looking like differential equations and more like optimizers. With this, we can experiment with using different loss functions like perceptual loss, etc. (Something to experiment with)"
  },
  {
    "objectID": "lesson_10/lesson_10.html",
    "href": "lesson_10/lesson_10.html",
    "title": "Stable Diffusion from the ground up",
    "section": "",
    "text": "Here we shall build a working Stable Diffusion model using just Python, the Python standard library, Matplotlib for plots, Jupyter Notebook, which is where we shall be writing our code, and nbdev, which we can use to create modules from notebooks.\n\nfrom pathlib import Path\nimport pickle, gzip, os, math, time, shutil , matplotlib as mpl, matplotlib.pyplot as plt\n\n\nGet data\n\nMNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\n\nfrom urllib.request import urlretrieve\n\n\nurlretrieve\n\n&lt;function urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)&gt;\n\n\n\nif not path_gz.exists(): urlretrieve(MNIST_URL,path_gz)\n\n\n!ls -l data\n\ntotal 16656\n-rw-r--r-- 1 rubanza rubanza 17051982 Jul 21 16:23 mnist.pkl.gz\n\n\n-rw-r–r– 1 user staff 15296311 Jul 21 10:30 mnist.pkl.gz | | | | | | | | | | | | | └── Filename | | | | | └── Modification time | | | | └── File size (bytes) | | | └── Group owner | | └── User owner\n| └── Number of hard links └── File permissions\n\npath_gz\n\nPosixPath('data/mnist.pkl.gz')\n\n\n\n#with gzip.open('data/mnist.pkl.gz','rb') as f:\n    #dataset = pickle.load(f, encoding='latin-1')\n\n\ndataset\n\n((array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([5, 0, 4, ..., 8, 4, 8])),\n (array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([3, 8, 6, ..., 5, 6, 8])),\n (array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([7, 2, 1, ..., 4, 5, 6])))\n\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f, encoding='latin-1')\n\n\nx_train.shape,y_train.shape,x_valid.shape,y_valid.shape\n\n((50000, 784), (50000,), (10000, 784), (10000,))\n\n\n\nx_train\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlen(lst1)\n\n784\n\n\n\nlen(lst1)\n\n784\n\n\n\ndef chunks(x,sz):\n    for i in range(0, len(x), sz): yield x[i:i+5]\n\n\nlist(chunks(vals,5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nval_iter = chunks(vals,5)\n\n\nval_iter\n\n&lt;generator object chunks at 0x715981808580&gt;\n\n\n\nnext(val_iter)\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nnext(val_iter)\n\nStopIteration: \n\n\n\nmpl.rcParams['image.cmap']='gray'\nplt.imshow(list(chunks(lst1,28)));\n\n\n\n\n\nplt.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)))\nplt.xlim(0, 28)  # Force x-axis to go 0-28\nplt.ylim(0, 28)  # Force y-axis to go 0-28\n\n(0.0, 28.0)\n\n\n\n\n\n\na = [1,2,3,4,5]\nlen(a)\n\n5\n\n\n\nfor i in range(0,5): \n    print(i+1)\n\n1\n2\n3\n4\n5\n\n\n\nfrom itertools import islice\n\n\nit = iter(vals)\n\n\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nit\n\n&lt;list_iterator at 0x715981848730&gt;\n\n\n\nnext(it)\n\n0.0\n\n\n\nnext(it),next(it),next(it)\n\n(0.0, 0.0, 0.19140625)\n\n\nislice\n\nit = iter(vals)\nislice(it,28)\n\n&lt;itertools.islice at 0x76d844786660&gt;\n\n\n\nisit = islice(it,5)\nisit\n\n&lt;itertools.islice at 0x7159819054e0&gt;\n\n\n\nnext(isit),next(isit),next(isit)\n\n(0.0, 0.0, 0.0)\n\n\n\nnext(isit)\n\n0.19140625\n\n\n\nnext(isit)\n\n0.9296875\n\n\n\nnext(isit)\n\nStopIteration: \n\n\n\nnext(isit)\n\n0.19140625\n\n\n\nlist(islice(it,5))\n\n[0.98828125, 0.98828125, 0.98828125]\n\n\n\nlist(islice(it,28))\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nlist(islice(it,28))\n\n[]\n\n\n\nnext(it)\n\nStopIteration: \n\n\n\nislice(it,2)\n\n&lt;itertools.islice at 0x715981af47c0&gt;\n\n\n\nlist(islice(it,5))\n\n[]\n\n\n\nlist(islice(it,5))\n\n[]\n\n\nusing lamda\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\nimg\n\n[[0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.01171875,\n  0.0703125,\n  0.0703125,\n  0.0703125,\n  0.4921875,\n  0.53125,\n  0.68359375,\n  0.1015625,\n  0.6484375,\n  0.99609375,\n  0.96484375,\n  0.49609375,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.1171875,\n  0.140625,\n  0.3671875,\n  0.6015625,\n  0.6640625,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.87890625,\n  0.671875,\n  0.98828125,\n  0.9453125,\n  0.76171875,\n  0.25,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.19140625,\n  0.9296875,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98046875,\n  0.36328125,\n  0.3203125,\n  0.3203125,\n  0.21875,\n  0.15234375,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0703125,\n  0.85546875,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.7734375,\n  0.7109375,\n  0.96484375,\n  0.94140625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.3125,\n  0.609375,\n  0.41796875,\n  0.98828125,\n  0.98828125,\n  0.80078125,\n  0.04296875,\n  0.0,\n  0.16796875,\n  0.6015625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0546875,\n  0.00390625,\n  0.6015625,\n  0.98828125,\n  0.3515625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.54296875,\n  0.98828125,\n  0.7421875,\n  0.0078125,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.04296875,\n  0.7421875,\n  0.98828125,\n  0.2734375,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.13671875,\n  0.94140625,\n  0.87890625,\n  0.625,\n  0.421875,\n  0.00390625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.31640625,\n  0.9375,\n  0.98828125,\n  0.98828125,\n  0.46484375,\n  0.09765625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.17578125,\n  0.7265625,\n  0.98828125,\n  0.98828125,\n  0.5859375,\n  0.10546875,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0625,\n  0.36328125,\n  0.984375,\n  0.98828125,\n  0.73046875,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.97265625,\n  0.98828125,\n  0.97265625,\n  0.25,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.1796875,\n  0.5078125,\n  0.71484375,\n  0.98828125,\n  0.98828125,\n  0.80859375,\n  0.0078125,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.15234375,\n  0.578125,\n  0.89453125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.9765625,\n  0.7109375,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.09375,\n  0.4453125,\n  0.86328125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.78515625,\n  0.3046875,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.08984375,\n  0.2578125,\n  0.83203125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.7734375,\n  0.31640625,\n  0.0078125,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0703125,\n  0.66796875,\n  0.85546875,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.76171875,\n  0.3125,\n  0.03515625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.21484375,\n  0.671875,\n  0.8828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.953125,\n  0.51953125,\n  0.04296875,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.53125,\n  0.98828125,\n  0.98828125,\n  0.98828125,\n  0.828125,\n  0.52734375,\n  0.515625,\n  0.0625,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0],\n [0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0,\n  0.0]]\n\n\n\nplt.imshow(img);\n\n\n\n\n\nlen(img)\n\n28\n\n\n\nit = iter(lst1)\ndef f(): return list(islice(it,28))\n\n\nf()\n\n[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.01171875,\n 0.0703125,\n 0.0703125,\n 0.0703125,\n 0.4921875,\n 0.53125,\n 0.68359375,\n 0.1015625,\n 0.6484375,\n 0.99609375,\n 0.96484375,\n 0.49609375,\n 0.0,\n 0.0,\n 0.0,\n 0.0]\n\n\n\n#img[28]\n\n\n\nMatrix and tensors\n\nimport torch\nfrom torch import tensor\n\n\nclass Matrix:\n    def __init__(self,xs): self.xs = xs\n    def __getitem__(self,idxs): return self.xs[idxs[0]][idxs[1]]\n\n\nimg[20][15]\n\nNameError: name 'img' is not defined\n\n\n\n#img[27,25]\n\n\nm = Matrix(img)\nm[20,15]\n\n\nb = [1,2,3]\nb\n\n\nb = tensor(b)\nb\n\n\nlen(img)\n\n\nimg_tens = tensor(img)\nimg_tens.shape\n\n\nimg_tens[20,15]\n\ntensor(0.9883)\n\n\n\ns = [1,2,3]\ns\n\n[1, 2, 3]\n\n\n\ndef s_to_int(a):\n    a = a+1\n    return a\n\n\nsa = list(map(s_to_int,s))\nsa\n\n[2, 3, 4]\n\n\n\n(x_train,y_train,x_valid,y_valid) = map(tensor, (x_train,y_train,x_valid,y_valid))\n\n\nx_train.shape,y_train.shape,x_valid.shape,y_valid.shape\n\n(torch.Size([50000, 784]),\n torch.Size([50000]),\n torch.Size([10000, 784]),\n torch.Size([10000]))\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\n\n\nTensor\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nimgs_a = x_train[0].reshape((-1,28,28))\nimgs_a.shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntyo = x_train[1].shape\n\n\n#plt.imshow(tyo)\n\n\nplt.imshow(imgs[0])\n\n&lt;matplotlib.image.AxesImage at 0x7ee7656fe4d0&gt;\n\n\n\n\n\n\nimgs[0]\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0117, 0.0703, 0.0703, 0.0703, 0.4922, 0.5312,\n         0.6836, 0.1016, 0.6484, 0.9961, 0.9648, 0.4961, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172,\n         0.1406, 0.3672, 0.6016, 0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883,\n         0.8789, 0.6719, 0.9883, 0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297,\n         0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805,\n         0.3633, 0.3203, 0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555,\n         0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n         0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.1367, 0.9414, 0.8789, 0.6250, 0.4219, 0.0039,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.3164, 0.9375, 0.9883, 0.9883, 0.4648,\n         0.0977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883,\n         0.5859, 0.1055, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844,\n         0.9883, 0.7305, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727,\n         0.9883, 0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n         0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883, 0.9883,\n         0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883, 0.9883, 0.7852,\n         0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0898,\n         0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.3164, 0.0078,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.6680, 0.8555,\n         0.9883, 0.9883, 0.9883, 0.9883, 0.7617, 0.3125, 0.0352, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.2148, 0.6719, 0.8828, 0.9883, 0.9883,\n         0.9883, 0.9883, 0.9531, 0.5195, 0.0430, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281,\n         0.5273, 0.5156, 0.0625, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]])\n\n\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\n\nn, c = x_train.shape\nn,c\n\n(50000, 784)\n\n\n\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\ny_train[0].shape\n\ntorch.Size([])\n\n\n\nmin(y_train), max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\n\nRandom numbers\nBased on the Wichmann Hill algorithm\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a,30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(42)\nrnd_state\n\n(43, 1, 1)\n\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x, y, z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(),rand()\n\n(0.25420336316883324, 0.46884405296716114)\n\n\n\nrand()\n\n0.19540525690312815\n\n\n\nrand(),rand(),rand(),rand()\n\n(0.28886109883281286,\n 0.8643955691976015,\n 0.062341103558347655,\n 0.5214729908496198)\n\n\n\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f' In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.44425801591077185\n In child: 0.44425801591077185\n\n\n\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f' In child: {torch.rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: tensor([0.9308])\n\n\n\nimport numpy as np\n\n\nif os.fork(): print(f'In parent: {np.random.rand(1)}')\nelse:\n    print(f' In child: {np.random.rand()}')\n    os._exit(os.EX_OK)\n\n In child: 0.6953703052450606\nIn parent: [0.69537031]\n\n\n\nfrom random import random\n\n\nif os.fork(): print(f'In parent: {random()}')\nelse:\n    print(f' In child: {random()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.006104636974047728\n In child: 0.22305777947091454\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.hist([rand() for _ in range(1000)]);"
  }
]